{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Significance Ranking\n",
    "- Based on: [Topic Significance Ranking of LDA Generative Models (Alsumait et al.)](https://mimno.infosci.cornell.edu/info6150/readings/ECML09_AlSumaitetal.pdf).\n",
    "- Data aquisition from [this website](https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/#3importnewsgroupstextdata) verbatum, with prints removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "def TopicSignificanceRanking(count_vector, components, documents):\n",
    "    '''\n",
    "    This takes the topics from an LDA model (sklearn) and assigns TSR scores to it.\n",
    "    \n",
    "        count_vector: the count vector of the words used in the sklearn model.\n",
    "    \n",
    "        components:   the topics generated by the LDA model.\n",
    "        \n",
    "        RETURNS TSR for each topic\n",
    "    \n",
    "    It is highly probable that this could be done easier than below, and indeed, highly likely.  I was doing this as I read through the paper, and therefore was thinking in terms of matching the text, not in terms of efficiency, except where blindingly obvious.\n",
    "    Darin LaSota, 5/30/2018\n",
    "    '''\n",
    "    # Derived Quantities\n",
    "    topics = components.shape[0]  # number of topics\n",
    "    measures = 3\n",
    "    measure = ['KL','COR','COS']\n",
    "    \n",
    "    # Distributions...\n",
    "    # W-Uniform is a junk topic in which all the terms of the dictionary are equally probable\n",
    "    W_Uniform = np.ones(components.shape)/count_vector.shape[1]\n",
    "    # As an aside, would probability that word appears in the entire corpus be useful?\n",
    "    \n",
    "    # the vacuous semantic distribution (WVacuous), is deﬁned to be the empirical distribution of the sample set\n",
    "    W_Vacuous = components / components.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # the background topic (D-BGround) is found equally probable in all the documents.\n",
    "    D_BGround = np.ones(documents.shape)/documents.shape[0]\n",
    "    \n",
    "    # Construct U, V, and B for each topic k\n",
    "    U = np.zeros((topics, measures))\n",
    "    V = np.zeros((topics, measures))\n",
    "    B = np.zeros((topics, measures))\n",
    "   \n",
    "    for k in range(topics):\n",
    "        # KL = 0\n",
    "        U[k, 0] = entropy(components[k,:], W_Uniform[k,:])\n",
    "        V[k, 0] = entropy(components[k,:],  W_Vacuous[k,:])\n",
    "        # Averaging distance of each document's word to D-BGround\n",
    "        B[k, 0] = np.mean(entropy(np.array(documents[:,k]), D_BGround[:,k]))\n",
    "\n",
    "        # COR\n",
    "        U[k, 1] = np.correlate(components[k,:], W_Uniform[k,:])\n",
    "        V[k, 1] = np.correlate(components[k,:],  W_Vacuous[k,:])\n",
    "        # Averaging distance of each document's word to D-BGround\n",
    "        B[k, 1] = np.mean(np.correlate(documents[:,k], D_BGround[:,k], mode='valid'))\n",
    "\n",
    "        # COS\n",
    "        U[k, 2] = cosine(components[k,:].reshape(-1,1), W_Uniform[k,:].reshape(-1,1))\n",
    "        V[k, 2] = cosine(components[k,:].reshape(-1,1),  W_Vacuous[k,:].reshape(-1,1))\n",
    "        # Averaging distance of each document's word to D-BGround\n",
    "        B[k, 2] = np.mean(cosine(documents[:,k].reshape(-1,1),D_BGround[:,k].reshape(-1,1)))\n",
    "    \n",
    "    # 4.1 Standardization Proceedure\n",
    "    # (10) and (11)\n",
    "    U1 = np.zeros((topics, measures))\n",
    "    V1 = np.zeros((topics, measures))\n",
    "    B1 = np.zeros((topics, measures))\n",
    "    U2 = np.zeros((topics, measures))\n",
    "    V2 = np.zeros((topics, measures))\n",
    "    B2 = np.zeros((topics, measures))\n",
    "    for k in range(topics):\n",
    "        for m in range(measures):\n",
    "            # (10)\n",
    "            U1[k,m] = U[k,m] * (1 - U[k,m]/np.sum(U[:,m], axis=0))\n",
    "            V1[k,m] = V[k,m] * (1 - V[k,m]/np.sum(V[:,m], axis=0))\n",
    "            B1[k,m] = B[k,m] * (1 - B[k,m]/np.sum(B[:,m], axis=0))\n",
    "            # (11)\n",
    "            U2[k,m] = (U[k,m] - np.min(U[:,m]))/(np.max(U[:,m]) - np.min(U[:,m]))\n",
    "            V2[k,m] = (V[k,m] - np.min(V[:,m]))/(np.max(V[:,m]) - np.min(V[:,m]))\n",
    "            B2[k,m] = (B[k,m] - np.min(B[:,m]))/(np.max(B[:,m]) - np.min(B[:,m]))\n",
    "    \n",
    "    # correction term to deal with all the same answer\n",
    "    # only happens when C - Cmin = 0 since Cmax - Cmin = 0\n",
    "    U2 = np.nan_to_num(U2)\n",
    "    V2 = np.nan_to_num(V2)\n",
    "    B2 = np.nan_to_num(B2)\n",
    "    \n",
    "    # 4.2 Intra-Criterion Weighted Linear Combination\n",
    "    S1 = (U1 + V1 + B1)/3\n",
    "    S2 = (U2 + V2 + B2)/3\n",
    "    print(S1)\n",
    "    # 4.3 Inter-Criterion Weighted Combination\n",
    "    '''\n",
    "        no indication of the proper way to mcalculate psi in the literature, except for the line: \n",
    "        These weights are assumed to sum to 1 so that the total score remains bounded between zero and one.\n",
    "        so, setting them all equal to each other... 1/(topics*measures)\n",
    "    '''\n",
    "    psi = np.ones((topics, measures)) #/(3*measures*topics)\n",
    "    S = S1[:,2]*(psi[:,0] * S1[:,0] + psi[:,1]*S1[:,1])  #(13)\n",
    "    Psi = psi[:,0] * S2[:,0] + psi[:,1] * S2[:,1] + psi[:,2] * S2[:,2]\n",
    "    \n",
    "    # 4.4 The Final Topic Significance Score\n",
    "    return S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darin\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 9751)\n"
     ]
    }
   ],
   "source": [
    "# Data import on its own line\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "import re, nltk, gensim, spacy\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
    "\n",
    "\n",
    "# Convert to list\n",
    "data = df.content.values.tolist()\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
    "    return texts_out\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# Run in terminal: python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only Noun, Adj, Verb, Adverb\n",
    "data_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=10,                        # minimum reqd occurences of a word \n",
    "                             stop_words='english',             # remove stop words\n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "                             # max_features=50000,             # max number of uniq words\n",
    "                            )\n",
    "\n",
    "data_vectorized = vectorizer.fit_transform(data_lemmatized)\n",
    "count_vector = data_vectorized\n",
    "print(count_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darin\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 10\n",
      "iteration: 2 of max_iter: 10\n",
      "iteration: 3 of max_iter: 10\n",
      "iteration: 4 of max_iter: 10\n",
      "iteration: 5 of max_iter: 10\n",
      "iteration: 6 of max_iter: 10\n",
      "iteration: 7 of max_iter: 10\n",
      "iteration: 8 of max_iter: 10\n",
      "iteration: 9 of max_iter: 10\n",
      "iteration: 10 of max_iter: 10\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "\n",
    "model = LatentDirichletAllocation(n_components=40, verbose=1, max_iter=10)\n",
    "documents = model.fit_transform(count_vector)\n",
    "components = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[             nan   3.34591603e+01   5.74417149e-01]\n",
      " [             nan   5.37353056e+01   5.14957269e-01]\n",
      " [             nan   4.54890736e+01   5.43129126e-01]\n",
      " [             nan   5.74552398e+01   4.89390633e-01]\n",
      " [             nan   6.46428529e+01   4.37645344e-01]\n",
      " [             nan   9.63954683e+01   5.32409917e-01]\n",
      " [             nan   7.11378810e+01   5.48403930e-01]\n",
      " [             nan   3.11593148e+01   5.47476426e-01]\n",
      " [             nan   2.80381483e+01   5.34398087e-01]\n",
      " [             nan   5.34543186e+01   5.13578470e-01]\n",
      " [             nan   8.92701513e+01   4.83968107e-01]\n",
      " [             nan   1.52621659e+01   5.63477738e-01]\n",
      " [             nan   1.00465634e+02   4.89613990e-01]\n",
      " [             nan   9.45216811e+01   5.21055735e-01]\n",
      " [             nan   5.61296589e+01   4.97313581e-01]\n",
      " [             nan   1.14031439e+02   4.35888880e-01]\n",
      " [             nan   4.12287157e+01   5.35294365e-01]\n",
      " [             nan   1.32125860e+01   5.62851895e-01]\n",
      " [             nan   3.45133705e+01   5.23070614e-01]\n",
      " [             nan   1.20827650e+02   5.54823284e-01]\n",
      " [             nan   6.57779272e+01   5.34761075e-01]\n",
      " [             nan   4.25779986e+01   5.56359010e-01]\n",
      " [             nan   5.45015746e+01   5.54271510e-01]\n",
      " [             nan   4.11020113e+01   5.30302472e-01]\n",
      " [             nan   2.46871320e+01   5.13161871e-01]\n",
      " [             nan   1.31963239e+02   4.89208916e-01]\n",
      " [             nan   1.34867418e+02   4.46566830e-01]\n",
      " [             nan   1.84823915e+02   5.42927024e-01]\n",
      " [             nan   5.83591799e+01   5.14388795e-01]\n",
      " [             nan   1.70889559e+02   5.45983351e-01]\n",
      " [             nan   3.98553595e+01   5.90279252e-01]\n",
      " [             nan   2.55847112e+02   3.72340577e-01]\n",
      " [             nan   6.00867732e+01   4.84264698e-01]\n",
      " [             nan   7.81393083e+01   3.94192167e-01]\n",
      " [             nan   3.33644286e+01   4.81654752e-01]\n",
      " [             nan   3.09393464e+01   5.32577857e-01]\n",
      " [             nan   4.04398351e+02   5.61203787e-01]\n",
      " [             nan   5.09109700e+02   4.49443634e-01]\n",
      " [             nan   9.97225060e+01   4.80150068e-01]\n",
      " [             nan   5.75210925e+01   4.54048143e-01]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TSR = TopicSignificanceRanking(count_vector, components, documents)\n",
    "TSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Derived Quantities\n",
    "topics = components.shape[0]  # number of topics\n",
    "measures = 3\n",
    "measure = ['KL','COR','COS']\n",
    "\n",
    "# Distributions...\n",
    "# W-Uniform is a junk topic in which all the terms of the dictionary are equally probable\n",
    "W_Uniform = np.ones(components.shape)/count_vector.shape[1]\n",
    "# As an aside, would probability that word appears in the entire corpus be useful?\n",
    "\n",
    "# the vacuous semantic distribution (WVacuous), is deﬁned to be the empirical distribution of the sample set\n",
    "W_Vacuous = components / components.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# the background topic (D-BGround) is found equally probable in all the documents.\n",
    "D_BGround = np.ones(count_vector.shape)/count_vector.shape[0]\n",
    "\n",
    "# Construct U, V, and B for each topic k\n",
    "U = np.zeros((topics, measures))\n",
    "V = np.zeros((topics, measures))\n",
    "B = np.zeros((topics, measures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00010255,  0.00010255,  0.00010255, ...,  0.00010255,\n",
       "         0.00010255,  0.00010255],\n",
       "       [ 0.00010255,  0.00010255,  0.00010255, ...,  0.00010255,\n",
       "         0.00010255,  0.00010255],\n",
       "       [ 0.00010255,  0.00010255,  0.00010255, ...,  0.00010255,\n",
       "         0.00010255,  0.00010255],\n",
       "       ..., \n",
       "       [ 0.00010255,  0.00010255,  0.00010255, ...,  0.00010255,\n",
       "         0.00010255,  0.00010255],\n",
       "       [ 0.00010255,  0.00010255,  0.00010255, ...,  0.00010255,\n",
       "         0.00010255,  0.00010255],\n",
       "       [ 0.00010255,  0.00010255,  0.00010255, ...,  0.00010255,\n",
       "         0.00010255,  0.00010255]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_Uniform = np.ones(components.shape)/count_vector.shape[1]\n",
    "W_Uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, topic in enumerate(TSR >= np.max(TSR)/3):\n",
    "    if topic == True:\n",
    "        print()\n",
    "        print('TOPIC {}'.format(i))\n",
    "        print(TSR[i])\n",
    "        for j, val in enumerate(components[i,:] >= np.max(components[i,:])/2):\n",
    "            if val == True:\n",
    "                print(vectorizer.get_feature_names()[j], components[i,j])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validity Testing\n",
    "First, test multiple runs of same number of topics (dif max_iter) then on dif topic numbers, to see if topic numbers have a greater effect on the mean than the 'actual' significance of the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = [1+a for a in range(8)]\n",
    "Y = []\n",
    "Ym = {}\n",
    "for x in X:\n",
    "    model = LatentDirichletAllocation(n_components=40, max_iter=x)\n",
    "    documents = model.fit_transform(count_vector)\n",
    "    components = model.components_\n",
    "    TSR = TopicSignificanceRanking(count_vector, components, documents)\n",
    "    Ym[x] = np.sort(TSR)\n",
    "    print(np.mean(TSR), TSR)\n",
    "    Y.append(np.mean(TSR))\n",
    "\n",
    "plt.plot(X, Y)\n",
    "plt.xlabel('max_iterations')\n",
    "plt.ylabel('mean TSR')\n",
    "plt.show()\n",
    "print(Ym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
