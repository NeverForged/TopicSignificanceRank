{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Significance Ranking\n",
    "- Based on: [Topic Significance Ranking of LDA Generative Models (Alsumait et al.)](https://mimno.infosci.cornell.edu/info6150/readings/ECML09_AlSumaitetal.pdf).\n",
    "- Data aquisition from [this website](https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/#3importnewsgroupstextdata) verbatum, with prints removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "def TopicSignificanceRanking(count_vector, components, documents):\n",
    "    '''\n",
    "    This takes the topics from an LDA model (sklearn) and assigns TSR scores to it.\n",
    "    \n",
    "        count_vector: the count vector of the words used in the sklearn model.\n",
    "    \n",
    "        components:   the topics generated by the LDA model.\n",
    "        \n",
    "        RETURNS Psi and S for each topic (where TSR = Psi x S)\n",
    "        \n",
    "    It is highly probable that this could be done easier than below, and indeed, highly likely.  I was doing this as I read through the paper, and therefore was thinking in terms of matching the text, not in terms of efficiency, except where blindingly obvious.\n",
    "    Darin LaSota, 5/30/2018\n",
    "    '''\n",
    "    # Turn components into word topic dist\n",
    "    components = components/components.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Derived Quantities\n",
    "    topics = components.shape[0]  # number of topics\n",
    "    measures = 3\n",
    "    measure = ['KL','COR','COS']\n",
    "    \n",
    "    # Distributions...\n",
    "    # W-Uniform is a junk topic in which all the terms of the dictionary are equally probable\n",
    "    W_Uniform = np.ones(components.shape)/components.shape[1]\n",
    "    \n",
    "    # the vacuous semantic distribution (W_Vacuous), is deﬁned to be the empirical distribution of the sample set\n",
    "    # p(w|W_Vacuous) = Sum Over all Topics, k of p(w|k)p(k)\n",
    "    W_Vacuous = np.zeros(components.shape)\n",
    "    p_k = np.sum(documents, axis=0)/np.sum(documents)\n",
    "    temp = np.zeros(components[0,:].shape)\n",
    "    for k in range(topics): \n",
    "        temp = temp + components[k,:] * p_k[k]  # (2)   \n",
    "    for k in range(topics):\n",
    "        W_Vacuous[k,:] = temp\n",
    "\n",
    "    # the background topic (D-BGround) is found equally probable in all the documents.\n",
    "    D_BGround = np.ones(documents.shape)/documents.shape[0]\n",
    "    \n",
    "    # Construct U, V, and B for each topic k\n",
    "    U = np.zeros((topics, measures))\n",
    "    V = np.zeros((topics, measures))\n",
    "    B = np.zeros((topics, measures))\n",
    "   \n",
    "    for k in range(topics):\n",
    "        # KL = 0\n",
    "        U[k, 0] = entropy(components[k,:], W_Uniform[k,:])\n",
    "        V[k, 0] = entropy(components[k,:],  W_Vacuous[k,:])\n",
    "        # Averaging distance of each document's word to D-BGround\n",
    "        B[k, 0] = np.mean(entropy(np.array(documents[:,k]), D_BGround[:,k]))\n",
    "\n",
    "        # COR\n",
    "        U[k, 1] = np.correlate(components[k,:], W_Uniform[k,:])\n",
    "        V[k, 1] = np.correlate(components[k,:],  W_Vacuous[k,:])\n",
    "        # Averaging distance of each document's word to D-BGround\n",
    "        B[k, 1] = np.mean(np.correlate(documents[:,k], D_BGround[:,k], mode='valid'))\n",
    "\n",
    "        # COS\n",
    "        U[k, 2] = cosine(components[k,:].reshape(-1,1), W_Uniform[k,:].reshape(-1,1))\n",
    "        V[k, 2] = cosine(components[k,:].reshape(-1,1),  W_Vacuous[k,:].reshape(-1,1))\n",
    "        # Averaging distance of each document's word to D-BGround\n",
    "        B[k, 2] = np.mean(cosine(documents[:,k].reshape(-1,1),D_BGround[:,k].reshape(-1,1)))\n",
    "   \n",
    "    # 4.1 Standardization Proceedure\n",
    "    # (10) and (11)\n",
    "    U1 = np.zeros((topics, measures))\n",
    "    V1 = np.zeros((topics, measures))\n",
    "    B1 = np.zeros((topics, measures))\n",
    "    U2 = np.zeros((topics, measures))\n",
    "    V2 = np.zeros((topics, measures))\n",
    "    B2 = np.zeros((topics, measures))\n",
    "    for k in range(topics):\n",
    "        for m in range(measures):\n",
    "            # (10)\n",
    "            U1[k,m] = U[k,m] * (1 - U[k,m]/np.sum(U[:,m], axis=0))\n",
    "            V1[k,m] = V[k,m] * (1 - V[k,m]/np.sum(V[:,m], axis=0))\n",
    "            B1[k,m] = B[k,m] * (1 - B[k,m]/np.sum(B[:,m], axis=0))\n",
    "            # (11)\n",
    "            U2[k,m] = (U[k,m] - np.min(U[:,m]))/(np.max(U[:,m]) - np.min(U[:,m]))\n",
    "            V2[k,m] = (V[k,m] - np.min(V[:,m]))/(np.max(V[:,m]) - np.min(V[:,m]))\n",
    "            B2[k,m] = (B[k,m] - np.min(B[:,m]))/(np.max(B[:,m]) - np.min(B[:,m]))\n",
    "    \n",
    "    # correction term to deal with all the same answer\n",
    "    # only happens when C - Cmin = 0 since Cmax - Cmin = 0\n",
    "    U1 = np.nan_to_num(U1)\n",
    "    V1 = np.nan_to_num(V1)\n",
    "    B1 = np.nan_to_num(B1)\n",
    "    U2 = np.nan_to_num(U2)\n",
    "    V2 = np.nan_to_num(V2)\n",
    "    B2 = np.nan_to_num(B2)\n",
    "    \n",
    "    # 4.2 Intra-Criterion Weighted Linear Combination\n",
    "    S1_U = (U1[:,0] + U1[:,1] + U1[:,2])/3  # (12)\n",
    "    S1_V = (V1[:,0] + V1[:,1] + V1[:,2])/3  # (12)\n",
    "    S1_B = (B1[:,0] + B1[:,1] + B1[:,2])/3  # (12)\n",
    "    S2_U = (U2[:,0] + U2[:,1] + U2[:,2])/3  # (12)\n",
    "    S2_V = (V2[:,0] + V2[:,1] + V2[:,2])/3  # (12)\n",
    "    S2_B = (B2[:,0] + B2[:,1] + B2[:,2])/3  # (12)\n",
    "    \n",
    "    # 4.3 Inter-Criterion Weighted Combination\n",
    " \n",
    "    # The ﬁrst WC technique is based on Equation (10) and uses the standardized score of the background criterion \n",
    "    #    as a weight for the uniformity and vacuousness.\n",
    "    psi_U = 1 - S1_U/np.sum(S1_U)\n",
    "    psi_V = 1 - S1_V/np.sum(S1_V)\n",
    "    S = S1_B*( psi_U * S1_U + psi_V * S1_V)   # (13)\n",
    "    \n",
    "    # These weights are assumed to sum to 1 so that the total score remains bounded between zero and one.\n",
    "    Psi = (1/3) * S2_U + (1/3) * S2_V + (1/3) * S2_B  # (14)\n",
    "    \n",
    "    # 4.4 The Final Topic Significance Score\n",
    "    return Psi, S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data import on its own line\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "import re, nltk, gensim, spacy\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
    "\n",
    "\n",
    "# Convert to list\n",
    "data = df.content.values.tolist()\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
    "    return texts_out\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# Run in terminal: python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only Noun, Adj, Verb, Adverb\n",
    "data_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=10,                        # minimum reqd occurences of a word \n",
    "                             stop_words='english',             # remove stop words\n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "                             # max_features=50000,             # max number of uniq words\n",
    "                            )\n",
    "\n",
    "data_vectorized = vectorizer.fit_transform(data_lemmatized)\n",
    "count_vector = data_vectorized\n",
    "print(count_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "\n",
    "model = LatentDirichletAllocation(n_components=40, max_iter=100)\n",
    "documents = model.fit_transform(count_vector)\n",
    "components = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Psi, S = TopicSignificanceRanking(count_vector, components, documents)\n",
    "print(Psi)\n",
    "print(S)\n",
    "TSR = Psi*S\n",
    "print(TSR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in list(TSR.argsort()[-5:][::-1]):\n",
    "    print()\n",
    "    print('TOPIC {}'.format(i))\n",
    "    print(' [TSR',TSR[i],'Psi',Psi[i],'S',S[i],']')\n",
    "    for j in list(components[i,:].argsort()[-10:][::-1]):\n",
    "        print('   ',vectorizer.get_feature_names()[j], '(',components[i,j],')')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in list(TSR.argsort()[:5][::-1]):\n",
    "    print()\n",
    "    print('TOPIC {}'.format(i))\n",
    "    print(' [TSR',TSR[i],'Psi',Psi[i],'S',S[i],']')\n",
    "    for j in list(components[i,:].argsort()[-10:][::-1]):\n",
    "        print('   ',vectorizer.get_feature_names()[j], '(',components[i,j],')')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validity Testing\n",
    "First, test multiple runs of same number of topics (dif max_iter) then on dif topic numbers, to see if topic numbers have a greater effect on the mean than the 'actual' significance of the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "print('hi')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = [10 + 5*a for a in range(20)]\n",
    "Y = []\n",
    "Ym = {}\n",
    "Ym[0] = []\n",
    "Ym[1] = []\n",
    "Ym[2] = []\n",
    "Ym[3] = []\n",
    "Ym[4] = []\n",
    "clear_output(wait=True)\n",
    "print('{:.1f}% Finished'.format(0))\n",
    "train, test, y1, y2 = train_test_split(count_vector, np.ones(count_vector.shape[0]),test_size=0.2)\n",
    "for i,x in enumerate(X):\n",
    "    model = LatentDirichletAllocation(n_components=x, max_iter=10)\n",
    "    model.fit(train)\n",
    "    documents = model.transform(train)\n",
    "    components = model.components_\n",
    "    Psi, S = TopicSignificanceRanking(train, components, documents)\n",
    "    TSR = Psi*S\n",
    "    Ym[0].append(model.score(test))\n",
    "    Ym[1].append(model.perplexity(test))\n",
    "    Ym[2].append(np.mean(Psi))\n",
    "    Ym[3].append(np.mean(S))\n",
    "    Ym[4].append(np.mean(S)*np.mean(Psi))\n",
    "    clear_output(wait=True)\n",
    "    print('{:.1f}% Finished'.format(100*(i+1)/len(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 1, sharex=True, figsize=(8,8))\n",
    "ylabs = ['Log-Likliehood', 'Perplexity', 'Mean Psi', 'Mean S', 'Mean TSR']\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.plot(X, Ym[i])\n",
    "    ax.set_ylabel(ylabs[i])\n",
    "    if i == len(axes.flatten())-1:\n",
    "        ax.set_xlabel('n_components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
