{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Significance Ranking\n",
    "- Based on: [Topic Significance Ranking of LDA Generative Models (Alsumait et al.)](https://mimno.infosci.cornell.edu/info6150/readings/ECML09_AlSumaitetal.pdf).\n",
    "- Data aquisition from [this website](https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/#3importnewsgroupstextdata) verbatum, with prints removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "def TopicSignificanceRanking(count_vector, components, documents):\n",
    "    '''\n",
    "    This takes the topics from an LDA model (sklearn) and assigns TSR scores to it.\n",
    "    \n",
    "        count_vector: the count vector of the words used in the sklearn model.\n",
    "    \n",
    "        components:   the topics generated by the LDA model.\n",
    "        \n",
    "        RETURNS Psi and S for each topic (where TSR = Psi x S)\n",
    "        \n",
    "    It is highly probable that this could be done easier than below, and indeed, highly likely.  I was doing this as I read through the paper, and therefore was thinking in terms of matching the text, not in terms of efficiency, except where blindingly obvious.\n",
    "    Darin LaSota, 5/30/2018\n",
    "    '''\n",
    "    # Turn components into word topic dist\n",
    "    components = components/components.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Derived Quantities\n",
    "    topics = components.shape[0]  # number of topics\n",
    "    measures = 3\n",
    "    measure = ['KL','COR','COS']\n",
    "    \n",
    "    # Distributions...\n",
    "    # W-Uniform is a junk topic in which all the terms of the dictionary are equally probable\n",
    "    W_Uniform = np.ones(components.shape)/components.shape[1]\n",
    "    \n",
    "    # the vacuous semantic distribution (W_Vacuous), is deﬁned to be the empirical distribution of the sample set\n",
    "    # p(w|W_Vacuous) = Sum Over all Topics, k of p(w|k)p(k)\n",
    "    W_Vacuous = np.zeros(components.shape)\n",
    "    p_k = np.sum(documents, axis=0)/np.sum(documents)\n",
    "    temp = np.zeros(components[0,:].shape)\n",
    "    for k in range(topics): \n",
    "        temp = temp + components[k,:] * p_k[k]  # (2)   \n",
    "    for k in range(topics):\n",
    "        W_Vacuous[k,:] = temp\n",
    "\n",
    "    # the background topic (D-BGround) is found equally probable in all the documents.\n",
    "    D_BGround = np.ones(documents.shape)/documents.shape[0]\n",
    "    \n",
    "    # Construct U, V, and B for each topic k\n",
    "    U = np.zeros((topics, measures))\n",
    "    V = np.zeros((topics, measures))\n",
    "    B = np.zeros((topics, measures))\n",
    "   \n",
    "    for k in range(topics):\n",
    "        # KL = 0\n",
    "        U[k, 0] = entropy(components[k,:], W_Uniform[k,:])\n",
    "        V[k, 0] = entropy(components[k,:],  W_Vacuous[k,:])\n",
    "        # Averaging distance of each document's word to D-BGround\n",
    "        B[k, 0] = np.mean(entropy(np.array(documents[:,k]), D_BGround[:,k]))\n",
    "\n",
    "        # COR\n",
    "        U[k, 1] = np.correlate(components[k,:], W_Uniform[k,:])\n",
    "        V[k, 1] = np.correlate(components[k,:],  W_Vacuous[k,:])\n",
    "        # Averaging distance of each document's word to D-BGround\n",
    "        B[k, 1] = np.mean(np.correlate(documents[:,k], D_BGround[:,k], mode='valid'))\n",
    "\n",
    "        # COS\n",
    "        U[k, 2] = cosine(components[k,:].reshape(-1,1), W_Uniform[k,:].reshape(-1,1))\n",
    "        V[k, 2] = cosine(components[k,:].reshape(-1,1),  W_Vacuous[k,:].reshape(-1,1))\n",
    "        # Averaging distance of each document's word to D-BGround\n",
    "        B[k, 2] = np.mean(cosine(documents[:,k].reshape(-1,1),D_BGround[:,k].reshape(-1,1)))\n",
    "   \n",
    "    # 4.1 Standardization Proceedure\n",
    "    # (10) and (11)\n",
    "    U1 = np.zeros((topics, measures))\n",
    "    V1 = np.zeros((topics, measures))\n",
    "    B1 = np.zeros((topics, measures))\n",
    "    U2 = np.zeros((topics, measures))\n",
    "    V2 = np.zeros((topics, measures))\n",
    "    B2 = np.zeros((topics, measures))\n",
    "    for k in range(topics):\n",
    "        for m in range(measures):\n",
    "            # (10)\n",
    "            U1[k,m] = U[k,m] * (1 - U[k,m]/np.sum(U[:,m], axis=0))\n",
    "            V1[k,m] = V[k,m] * (1 - V[k,m]/np.sum(V[:,m], axis=0))\n",
    "            B1[k,m] = B[k,m] * (1 - B[k,m]/np.sum(B[:,m], axis=0))\n",
    "            # (11)\n",
    "            U2[k,m] = (U[k,m] - np.min(U[:,m]))/(np.max(U[:,m]) - np.min(U[:,m]))\n",
    "            V2[k,m] = (V[k,m] - np.min(V[:,m]))/(np.max(V[:,m]) - np.min(V[:,m]))\n",
    "            B2[k,m] = (B[k,m] - np.min(B[:,m]))/(np.max(B[:,m]) - np.min(B[:,m]))\n",
    "    \n",
    "    # correction term to deal with all the same answer\n",
    "    # only happens when C - Cmin = 0 since Cmax - Cmin = 0\n",
    "    U1 = np.nan_to_num(U1)\n",
    "    V1 = np.nan_to_num(V1)\n",
    "    B1 = np.nan_to_num(B1)\n",
    "    U2 = np.nan_to_num(U2)\n",
    "    V2 = np.nan_to_num(V2)\n",
    "    B2 = np.nan_to_num(B2)\n",
    "    \n",
    "    # 4.2 Intra-Criterion Weighted Linear Combination\n",
    "    S1_U = (U1[:,0] + U1[:,1] + U1[:,2])/3  # (12)\n",
    "    S1_V = (V1[:,0] + V1[:,1] + V1[:,2])/3  # (12)\n",
    "    S1_B = (B1[:,0] + B1[:,1] + B1[:,2])/3  # (12)\n",
    "    S2_U = (U2[:,0] + U2[:,1] + U2[:,2])/3  # (12)\n",
    "    S2_V = (V2[:,0] + V2[:,1] + V2[:,2])/3  # (12)\n",
    "    S2_B = (B2[:,0] + B2[:,1] + B2[:,2])/3  # (12)\n",
    "    \n",
    "    # 4.3 Inter-Criterion Weighted Combination\n",
    " \n",
    "    # The ﬁrst WC technique is based on Equation (10) and uses the standardized score of the background criterion \n",
    "    #    as a weight for the uniformity and vacuousness.\n",
    "    psi_U = 1 - S1_U/np.sum(S1_U)\n",
    "    psi_V = 1 - S1_V/np.sum(S1_V)\n",
    "    S = S1_B*( psi_U * S1_U + psi_V * S1_V)   # (13)\n",
    "    \n",
    "    # These weights are assumed to sum to 1 so that the total score remains bounded between zero and one.\n",
    "    Psi = (1/3) * S2_U + (1/3) * S2_V + (1/3) * S2_B  # (14)\n",
    "    \n",
    "    # 4.4 The Final Topic Significance Score\n",
    "    return Psi, S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darin\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e53dcf7c76b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[1;31m# Do lemmatization keeping only Noun, Adj, Verb, Adverb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mdata_lemmatized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlemmatization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallowed_postags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'NOUN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ADJ'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'VERB'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ADV'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-e53dcf7c76b2>\u001b[0m in \u001b[0;36mlemmatization\u001b[0;34m(texts, allowed_postags)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mtexts_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mtexts_out\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'-PRON-'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m''\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mallowed_postags\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtexts_out\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Darin\\Anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable)\u001b[0m\n\u001b[1;32m    344\u001b[0m             raise ValueError(Errors.E088.format(length=len(text),\n\u001b[1;32m    345\u001b[0m                                                 max_length=self.max_length))\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Darin\\Anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36mmake_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmake_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgolds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mtokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._tokenize\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mtokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._attach_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mvocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab.get\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mvocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab._new_lexeme\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Darin\\Anaconda3\\lib\\site-packages\\spacy\\lang\\lex_attrs.py\u001b[0m in \u001b[0;36mis_alpha\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msuffix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcluster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m \u001b[1;32mdef\u001b[0m \u001b[0mis_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mis_digit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mis_lower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mislower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Data import on its own line\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "import re, nltk, gensim, spacy\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
    "\n",
    "\n",
    "# Convert to list\n",
    "data = df.content.values.tolist()\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
    "    return texts_out\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# Run in terminal: python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only Noun, Adj, Verb, Adverb\n",
    "data_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=10,                        # minimum reqd occurences of a word \n",
    "                             stop_words='english',             # remove stop words\n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "                             # max_features=50000,             # max number of uniq words\n",
    "                            )\n",
    "\n",
    "data_vectorized = vectorizer.fit_transform(data_lemmatized)\n",
    "count_vector = data_vectorized\n",
    "print(count_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_vectorized' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f67554ea8f4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_vectorized\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data_vectorized' is not defined"
     ]
    }
   ],
   "source": [
    "data_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "\n",
    "model = LatentDirichletAllocation(n_components=40, max_iter=100)\n",
    "documents = model.fit_transform(count_vector)\n",
    "components = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Psi, S = TopicSignificanceRanking(count_vector, components, documents)\n",
    "print(Psi)\n",
    "print(S)\n",
    "TSR = Psi*S\n",
    "print(TSR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in list(TSR.argsort()[-5:][::-1]):\n",
    "    print()\n",
    "    print('TOPIC {}'.format(i))\n",
    "    print(' [TSR',TSR[i],'Psi',Psi[i],'S',S[i],']')\n",
    "    for j in list(components[i,:].argsort()[-10:][::-1]):\n",
    "        print('   ',vectorizer.get_feature_names()[j], '(',components[i,j],')')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in list(TSR.argsort()[:5][::-1]):\n",
    "    print()\n",
    "    print('TOPIC {}'.format(i))\n",
    "    print(' [TSR',TSR[i],'Psi',Psi[i],'S',S[i],']')\n",
    "    for j in list(components[i,:].argsort()[-10:][::-1]):\n",
    "        print('   ',vectorizer.get_feature_names()[j], '(',components[i,j],')')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validity Testing\n",
    "Comparison of method here to method [here](https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/), noting that Perplexity and Log-Likliehood seem to rise/fall with n_components, while TSR has some sort of maximum in the middle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GraphTSR(dct):\n",
    "    xs = list(dct.keys())\n",
    "    xs.sort()\n",
    "    x = []\n",
    "    y = {}\n",
    "    yn  = {}\n",
    "    ym = {}\n",
    "    yt = {}\n",
    "\n",
    "    y_labels = ['Log-Likliehood', 'Perplexity', 'Psi', 'S', 'Mean TSR']\n",
    "    y[0] = []\n",
    "    y[1] = [] \n",
    "    y[2] = []\n",
    "    y[3] = []\n",
    "    y[4] = []\n",
    "    yn[0] = []\n",
    "    yn[1] = [] \n",
    "    yn[2] = []\n",
    "    yn[3] = []\n",
    "    yn[4] = []\n",
    "    ym[0] = []\n",
    "    ym[1] = [] \n",
    "    ym[2] = []\n",
    "    ym[3] = []\n",
    "    ym[4] = []\n",
    "    yt[0] = []\n",
    "    ym[1] = [] \n",
    "    ym[2] = []\n",
    "    ym[3] = []\n",
    "    ym[4] = []\n",
    "    for i in xs:\n",
    "        if len(dct[i]['log']) >=1:\n",
    "            x.append(i)\n",
    "            y[0].append(np.mean(dct[i]['log']))\n",
    "            y[1].append(np.mean(dct[i]['perplexity']))\n",
    "            y[2].append(np.mean(dct[i]['Psi']))\n",
    "            y[3].append(np.mean(dct[i]['S']))\n",
    "            y[4].append(np.mean(dct[i]['TSR']))\n",
    "\n",
    "            yn[0].append(np.mean(dct[i]['log']) - (np.std(dct[i]['log'])/(len(dct[i]['log'])**0.5)))\n",
    "            yn[1].append(np.mean(dct[i]['perplexity']) - (np.std(dct[i]['perplexity'])/(len(dct[i]['perplexity'])**0.5)))\n",
    "            yn[2].append(np.mean(dct[i]['Psi']) - (np.std(dct[i]['Psi'])/(len(dct[i]['Psi'])**0.5)))\n",
    "            yn[3].append(np.mean(dct[i]['S']) - (np.std(dct[i]['S'])/(len(dct[i]['S'])**0.5)))\n",
    "            yn[4].append(np.mean(dct[i]['TSR']) - (np.std(dct[i]['TSR'])/(len(dct[i]['TSR'])**0.5)))\n",
    "\n",
    "            ym[0].append(np.mean(dct[i]['log']) + (np.std(dct[i]['log'])/(len(dct[i]['log'])**0.5)))\n",
    "            ym[1].append(np.mean(dct[i]['perplexity']) + (np.std(dct[i]['perplexity'])/(len(dct[i]['perplexity'])**0.5)))\n",
    "            ym[2].append(np.mean(dct[i]['Psi']) + (np.std(dct[i]['Psi'])/(len(dct[i]['Psi'])**0.5)))\n",
    "            ym[3].append(np.mean(dct[i]['S']) + (np.std(dct[i]['S'])/(len(dct[i]['S'])**0.5)))\n",
    "            ym[4].append(np.mean(dct[i]['TSR']) + (np.std(dct[i]['TSR'])/(len(dct[i]['TSR'])**0.5)))\n",
    "\n",
    "    with plt.xkcd():\n",
    "    # if True:\n",
    "        fig, axes = plt.subplots(5,1,sharex=True,figsize=(8,12))\n",
    "        for i, ax in enumerate(axes.flatten()):\n",
    "            best=[]\n",
    "            if i == 1:\n",
    "                ax.axhline(np.min(y[i]), xmin=0, xmax=1, c='r', alpha=0.75, \n",
    "                           label='min {} = {:.2f}'.format(y_labels[i],np.min(y[i])))\n",
    "                ax.axvline(x[y[i].index(np.min(y[i]))],0,0, c='k', ls='', \n",
    "                           label='Best # Topics: {}'.format(x[y[i].index(np.min(y[i]))]))\n",
    "                ax.legend(loc='upper center', bbox_to_anchor=(0,.2,1,0.8), \n",
    "                      fancybox=True, shadow=False, ncol=2, fontsize='small')\n",
    "                ax.axhline(yn[i][y[i].index(np.min(y[i]))],0,1,linestyle=':',c='r',alpha=0.75)\n",
    "                ax.axhline(ym[i][y[i].index(np.min(y[i]))],0,1,linestyle=':',c='r',alpha=0.75)\n",
    "            else:\n",
    "                ax.axhline(np.max(y[i]), xmin=0, xmax=1, c='r', alpha=0.75, \n",
    "                           label='max {} = {:.4f}'.format(y_labels[i],np.max(y[i])))\n",
    "                ax.axvline(x[y[i].index(np.max(y[i]))],0,0, c='k', ls='', \n",
    "                           label='Best # Topics: {}'.format(x[y[i].index(np.max(y[i]))]))\n",
    "                ax.legend(loc='lower center', bbox_to_anchor=(0,0,1,0.2), \n",
    "                      fancybox=True, shadow=False, ncol=2, fontsize='small')\n",
    "\n",
    "                best_x = x[y[i].index(np.max(y[i]))]\n",
    "                best.append( best_x )\n",
    "                ax.axhline(yn[i][y[i].index(np.max(y[i]))],0,1,linestyle=':',c='r',alpha=0.75)\n",
    "                ax.axhline(ym[i][y[i].index(np.max(y[i]))],0,1,linestyle=':',c='r',alpha=0.75)\n",
    "                for a in ym[i]:\n",
    "                    if a > yn[i][y[i].index(np.max(y[i]))]:\n",
    "                        best.append(x[ym[i].index(a)])\n",
    "            ax.fill_between(x, yn[i], ym[i], color='b', alpha=0.5)\n",
    "            ax.plot(x,y[i], color='b', ls='solid', alpha=.9)\n",
    "            ax.set_ylabel(y_labels[i])\n",
    "            if i == 4:\n",
    "                ax.set_xlabel('n_components')\n",
    "\n",
    "            ax.set_xlim(np.min(x), np.max(x))\n",
    "            # ax.tick_params(axis=u'both', which=u'both',length=2)\n",
    "            ax.set_xticks(x)\n",
    "    plt.show()\n",
    "    best = list(set(best))\n",
    "    best.sort()\n",
    "    print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "fold = KFold(5)\n",
    "j = 0\n",
    "for x, y in fold.split(X):\n",
    "    if j <= 1:\n",
    "        for n in range(10):\n",
    "            try:\n",
    "                dct = pickle.load(open('gridct','rb'))\n",
    "            except:\n",
    "                dct = {}\n",
    "            print(dct.keys())\n",
    "              # SET IT HERE\n",
    "            n_components =  10*n\n",
    "            try:\n",
    "                len(dct[n_components])\n",
    "            except:\n",
    "                print('-New Data Point-')\n",
    "                dct[n_components] = {}\n",
    "                dct[n_components]['Psi'] = []\n",
    "                dct[n_components]['S'] = []\n",
    "                dct[n_components]['TSR'] = []\n",
    "                dct[n_components]['log'] = []\n",
    "                dct[n_components]['perplexity'] = []\n",
    "            print(dct.keys())\n",
    "            max_iter = 3\n",
    "            model = LatentDirichletAllocation(n_components=n_components, \n",
    "                                              max_iter=max_iter, \n",
    "                                              n_jobs=-1,\n",
    "                                              verbose=1,\n",
    "                                              learning_method='online')\n",
    "            model.fit(X[x])\n",
    "            print('model run')\n",
    "            Y = X[y]\n",
    "            components = model.components_\n",
    "            documents = model.transform(Y)\n",
    "            Psi, S = TopicSignificanceRanking(Y, components, documents)\n",
    "            dct[n_components]['Psi'].append(np.mean(Psi))\n",
    "            dct[n_components]['S'].append(np.mean(S))\n",
    "            dct[n_components]['TSR'].append(np.mean(Psi*S))\n",
    "            dct[n_components]['log'].append(model.score(Y))\n",
    "            print(\"Log Likelyhood {}\".format(model.score(Y)))\n",
    "            dct[n_components]['perplexity'].append(model.perplexity(Y))\n",
    "            print(\"Perplexity {}\".format(model.perplexity(Y)))\n",
    "            print('Mean TSR {}'.format(np.mean(Psi*S)))\n",
    "            pickle.dump(dct, open('gridct','wb'))\n",
    "            clear_output(wait=True)\n",
    "            if len(dct.keys()) > 1:\n",
    "                GraphTSR()\n",
    "            print('{:.1f}% Finished'.format( (20*j + i)/100)\n",
    "    j += 1"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
