{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Significance Ranking\n",
    "Based on: [Topic Significance Ranking of LDA Generative\n",
    "Models (Alsumait et al.)](https://mimno.infosci.cornell.edu/info6150/readings/ECML09_AlSumaitetal.pdf).\n",
    "\n",
    "Data aquisition from [this website](https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/#3importnewsgroupstextdata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "def TopicSignificanceRanking(count_vector, components, documents):\n",
    "    '''\n",
    "    This takes the topics from an LDA model (sklearn) and assigns TSR scores to it.\n",
    "    \n",
    "        count_vector: the count vector of the words used in the sklearn model.\n",
    "    \n",
    "        components:   the topics generated by the LDA model.\n",
    "        \n",
    "        RETURNS TSR for each topic\n",
    "    \n",
    "    It is highly probable that this could be done easier than below, and indeed, highly likely.  I was doing this as I read through the paper, and therefore was thinking in terms of matching the text, not in terms of efficiency, except where blindingly obvious.\n",
    "    Darin LaSota, 5/30/2018\n",
    "    '''\n",
    "    # Derived Quantities\n",
    "    topics = components.shape[0]  # number of topics\n",
    "    measures = 3\n",
    "    measure = ['KL','COR','COS']\n",
    "    \n",
    "    # Distributions...\n",
    "    # is it a word in the corpus?\n",
    "    uniform_distr = np.ones(components[0,:].shape)/count_vector.shape[1]\n",
    "    # is it as common in the corpus as it is in the dataset?\n",
    "    vacuous_distr = np.array(np.sum(count_vector, axis=0))[0]/np.mean(np.array(np.sum(count_vector, axis=0))[0])\n",
    "    # is it as common in the dataset as any document?\n",
    "    bground_distr = np.ones(count_vector[:,1].shape)/count_vector.shape[0]\n",
    "    \n",
    "    # Construct U, V, and B for each topic k\n",
    "    U = np.zeros((topics, measures))\n",
    "    V = np.zeros((topics, measures))\n",
    "    B = np.zeros((topics, measures))\n",
    "    for k in range(topics):\n",
    "        # KL = 0\n",
    "        U[k, 0] = entropy(components[k,:], uniform_distr)\n",
    "        V[k, 0] = entropy(components[k,:], vacuous_distr)\n",
    "        B[k, 0] = np.mean(entropy(np.array(documents[:,0]), bground_distr))\n",
    "        # COR\n",
    "        U[k, 1] = np.correlate(components[k,:], uniform_distr)\n",
    "        V[k, 1] = np.correlate(components[k,:], vacuous_distr)\n",
    "        B[k, 1] = np.mean(np.correlate(documents[:,k], bground_distr[:,0], mode='valid'))\n",
    "        # COS\n",
    "        U[k, 2] = cosine(components[k,:].reshape(-1,1), uniform_distr.reshape(-1,1))\n",
    "        V[k, 2] = cosine(components[k,:].reshape(-1,1), vacuous_distr.reshape(-1,1))\n",
    "        B[k, 2] = np.mean(cosine(documents[:,k].reshape(-1,1), bground_distr.reshape(-1,1)))\n",
    "    \n",
    "    # 4.1 Standardization Proceedure\n",
    "    # (10) and (11)\n",
    "    U1 = np.zeros((topics, measures))\n",
    "    V1 = np.zeros((topics, measures))\n",
    "    B1 = np.zeros((topics, measures))\n",
    "    U2 = np.zeros((topics, measures))\n",
    "    V2 = np.zeros((topics, measures))\n",
    "    B2 = np.zeros((topics, measures))\n",
    "    for k in range(topics):\n",
    "        for m in range(measures):\n",
    "            # (10)\n",
    "            U1[k,m] = U[k,m] * (1 - U[k,m]/np.sum(U[:,m], axis=0))\n",
    "            V1[k,m] = V[k,m] * (1 - V[k,m]/np.sum(V[:,m], axis=0))\n",
    "            B1[k,m] = B[k,m] * (1 - B[k,m]/np.sum(B[:,m], axis=0))\n",
    "            # (11)\n",
    "            U2[k,m] = (U[k,m] - np.min(U[:,m]))/(np.max(U[:,m]) - np.min(U[:,m]))\n",
    "            V2[k,m] = (V[k,m] - np.min(V[:,m]))/(np.max(V[:,m]) - np.min(V[:,m]))\n",
    "            B2[k,m] = (B[k,m] - np.min(B[:,m]))/(np.max(B[:,m]) - np.min(B[:,m]))\n",
    "    \n",
    "    # correction term to deal with all the same answer\n",
    "    # only happens when C - Cmin = 0 since Cmax - Cmin = 0\n",
    "    U2 = np.nan_to_num(U2)\n",
    "    V2 = np.nan_to_num(V2)\n",
    "    B2 = np.nan_to_num(B2)\n",
    "    \n",
    "    # 4.2 Intra-Criterion Weighted Linear Combination\n",
    "    S1 = (U1 + V1 + B1)/3\n",
    "    S2 = (U2 + V2 + B2)/3\n",
    "\n",
    "    # 4.3 Inter-Criterion Weighted Combination\n",
    "    '''\n",
    "        no indication of the proper way to mcalculate psi in the literature, except for the line: \n",
    "        These weights are assumed to sum to 1 so that the total score remains bounded between zero and one.\n",
    "        so, setting them all equal to each other... 1/(topics*measures)\n",
    "    '''\n",
    "    psi = np.ones((topics, measures))/(3*measures*topics)\n",
    "    S = S1[:,2]*(psi[:,0] * S1[:,0] + psi[:,1]*S1[:,1])  #(13)\n",
    "    Psi = psi[:,0] * S2[:,0] + psi[:,1] * S2[:,1] + psi[:,2] * S2[:,2]\n",
    "        \n",
    "    # 4.4 The Final Topic Significance Score\n",
    "    return Psi*S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-1dd6492a053a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;31m# Data import on its own line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import re, nltk, spacy, gensim\n",
    "\n",
    "# Data import on its own line\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
    "print(df.target_names.unique())\n",
    "\n",
    "\n",
    "# Convert to list\n",
    "data = df.content.values.tolist()\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "pprint(data[:1])\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
    "    return texts_out\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# Run in terminal: python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only Noun, Adj, Verb, Adverb\n",
    "data_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darin\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 10\n",
      "iteration: 2 of max_iter: 10\n",
      "iteration: 3 of max_iter: 10\n",
      "iteration: 4 of max_iter: 10\n",
      "iteration: 5 of max_iter: 10\n",
      "iteration: 6 of max_iter: 10\n",
      "iteration: 7 of max_iter: 10\n",
      "iteration: 8 of max_iter: 10\n",
      "iteration: 9 of max_iter: 10\n",
      "iteration: 10 of max_iter: 10\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "model = LatentDirichletAllocation(n_components=40, verbose=1, max_iter=10)\n",
    "documents = model.fit_transform(count_vector)\n",
    "components = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darin\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:68: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.00196096,  0.00469532,  0.00567782,  0.0018698 ,  0.00224714,\n",
       "        0.00194482,  0.00190373,  0.00177843,  0.00181902,  0.00235754,\n",
       "        0.00177843,  0.00208114,  0.00226861,  0.00177843,  0.00217533,\n",
       "        0.00482521,  0.00284637,  0.00227332,  0.00194099,  0.00179318,\n",
       "        0.00244928,  0.00239068,  0.11279445,  0.00179696,  0.00272435,\n",
       "        0.00177843,  0.00177843,  0.00177843,  0.00217827,  0.00177843,\n",
       "        0.00177843,  0.00177843,  0.00179317,  0.00192145,  0.00239094,\n",
       "        0.0023009 ,  0.00177843,  0.00177843,  0.00183746,  0.00183248])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TSR = TopicSignificanceRanking(count_vector, components, documents)\n",
    "TSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18846, 130107)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validity Testing\n",
    "First, test multiple runs of same number of topics (dif max_iter) then on dif topic numbers, to see if topic numbers have a greater effect on the mean than the 'actual' significance of the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darin\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\Darin\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:68: RuntimeWarning: invalid value encountered in double_scalars\n",
      "C:\\Users\\Darin\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1137574447 0.00499861139469\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-316ab73bf6f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLatentDirichletAllocation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdocuments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mcomponents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mTSR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTopicSignificanceRanking\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount_vector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Darin\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Darin\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    550\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0midx_slice\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgen_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                         self._em_step(X[idx_slice, :], total_samples=n_samples,\n\u001b[0;32m--> 552\u001b[0;31m                                       batch_update=False, parallel=parallel)\n\u001b[0m\u001b[1;32m    553\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                     \u001b[1;31m# batch update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Darin\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py\u001b[0m in \u001b[0;36m_em_step\u001b[0;34m(self, X, total_samples, batch_update, parallel)\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[1;31m# update `component_` related variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         self.exp_dirichlet_component_ = np.exp(\n\u001b[0;32m--> 450\u001b[0;31m             _dirichlet_expectation_2d(self.components_))\n\u001b[0m\u001b[1;32m    451\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_batch_iter_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = [1+a for a in range(8)]\n",
    "Y = []\n",
    "Ym = {}\n",
    "for x in X:\n",
    "    model = LatentDirichletAllocation(n_components=40, max_iter=x)\n",
    "    documents = model.fit_transform(count_vector)\n",
    "    components = model.components_\n",
    "    TSR = TopicSignificanceRanking(count_vector, components, documents)\n",
    "    Ym[x] = np.sort(TSR)\n",
    "    print(np.max(TSR), np.mean(TSR))\n",
    "    Y.append(np.mean(TSR))\n",
    "\n",
    "plt.plot(X, Y)\n",
    "plt.xlabel('max_iterations')\n",
    "plt.ylabel('mean TSR')\n",
    "plt.show()\n",
    "print(Ym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18846, 40)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
