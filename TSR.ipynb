{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Significance Ranking\n",
    "- Based on: [Topic Significance Ranking of LDA Generative Models (Alsumait et al.)](https://mimno.infosci.cornell.edu/info6150/readings/ECML09_AlSumaitetal.pdf).\n",
    "- Data aquisition from [this website](https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/#3importnewsgroupstextdata) verbatum, with prints removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "def TopicSignificanceRanking(count_vector, components, documents):\n",
    "    '''\n",
    "    This takes the topics from an LDA model (sklearn) and assigns TSR scores to it.\n",
    "    \n",
    "        count_vector: the count vector of the words used in the sklearn model.\n",
    "    \n",
    "        components:   the topics generated by the LDA model.\n",
    "        \n",
    "        RETURNS TSR for each topic\n",
    "    \n",
    "    It is highly probable that this could be done easier than below, and indeed, highly likely.  I was doing this as I read through the paper, and therefore was thinking in terms of matching the text, not in terms of efficiency, except where blindingly obvious.\n",
    "    Darin LaSota, 5/30/2018\n",
    "    '''\n",
    "    # Derived Quantities\n",
    "    topics = components.shape[0]  # number of topics\n",
    "    measures = 3\n",
    "    measure = ['KL','COR','COS']\n",
    "    \n",
    "    # Distributions...\n",
    "    # is it a word in the corpus?\n",
    "    uniform_distr = np.ones(components[0,:].shape)/count_vector.shape[1]\n",
    "    # is it as common in the corpus as it is in the dataset?\n",
    "    vacuous_distr = np.array(np.sum(count_vector, axis=0))[0]/np.mean(np.array(np.sum(count_vector, axis=0))[0])\n",
    "    # is it as common in the dataset as any document?\n",
    "    bground_distr = np.ones(count_vector[:,1].shape)/count_vector.shape[0]\n",
    "    \n",
    "    # Construct U, V, and B for each topic k\n",
    "    U = np.zeros((topics, measures))\n",
    "    V = np.zeros((topics, measures))\n",
    "    B = np.zeros((topics, measures))\n",
    "    for k in range(topics):\n",
    "        # KL = 0\n",
    "        U[k, 0] = entropy(components[k,:], uniform_distr)\n",
    "        V[k, 0] = entropy(components[k,:], vacuous_distr)\n",
    "        B[k, 0] = np.mean(entropy(np.array(documents[:,0]), bground_distr))\n",
    "        # COR\n",
    "        U[k, 1] = np.correlate(components[k,:], uniform_distr)\n",
    "        V[k, 1] = np.correlate(components[k,:], vacuous_distr)\n",
    "        B[k, 1] = np.mean(np.correlate(documents[:,k], bground_distr[:,0], mode='valid'))\n",
    "        # COS\n",
    "        U[k, 2] = cosine(components[k,:].reshape(-1,1), uniform_distr.reshape(-1,1))\n",
    "        V[k, 2] = cosine(components[k,:].reshape(-1,1), vacuous_distr.reshape(-1,1))\n",
    "        B[k, 2] = np.mean(cosine(documents[:,k].reshape(-1,1), bground_distr.reshape(-1,1)))\n",
    "    \n",
    "    # 4.1 Standardization Proceedure\n",
    "    # (10) and (11)\n",
    "    U1 = np.zeros((topics, measures))\n",
    "    V1 = np.zeros((topics, measures))\n",
    "    B1 = np.zeros((topics, measures))\n",
    "    U2 = np.zeros((topics, measures))\n",
    "    V2 = np.zeros((topics, measures))\n",
    "    B2 = np.zeros((topics, measures))\n",
    "    for k in range(topics):\n",
    "        for m in range(measures):\n",
    "            # (10)\n",
    "            U1[k,m] = U[k,m] * (1 - U[k,m]/np.sum(U[:,m], axis=0))\n",
    "            V1[k,m] = V[k,m] * (1 - V[k,m]/np.sum(V[:,m], axis=0))\n",
    "            B1[k,m] = B[k,m] * (1 - B[k,m]/np.sum(B[:,m], axis=0))\n",
    "            # (11)\n",
    "            U2[k,m] = (U[k,m] - np.min(U[:,m]))/(np.max(U[:,m]) - np.min(U[:,m]))\n",
    "            V2[k,m] = (V[k,m] - np.min(V[:,m]))/(np.max(V[:,m]) - np.min(V[:,m]))\n",
    "            B2[k,m] = (B[k,m] - np.min(B[:,m]))/(np.max(B[:,m]) - np.min(B[:,m]))\n",
    "    \n",
    "    # correction term to deal with all the same answer\n",
    "    # only happens when C - Cmin = 0 since Cmax - Cmin = 0\n",
    "    U2 = np.nan_to_num(U2)\n",
    "    V2 = np.nan_to_num(V2)\n",
    "    B2 = np.nan_to_num(B2)\n",
    "    \n",
    "    # 4.2 Intra-Criterion Weighted Linear Combination\n",
    "    S1 = (U1 + V1 + B1)/3\n",
    "    S2 = (U2 + V2 + B2)/3\n",
    "\n",
    "    # 4.3 Inter-Criterion Weighted Combination\n",
    "    '''\n",
    "        no indication of the proper way to mcalculate psi in the literature, except for the line: \n",
    "        These weights are assumed to sum to 1 so that the total score remains bounded between zero and one.\n",
    "        so, setting them all equal to each other... 1/(topics*measures)\n",
    "    '''\n",
    "    psi = np.ones((topics, measures))/(3*measures*topics)\n",
    "    S = S1[:,2]*(psi[:,0] * S1[:,0] + psi[:,1]*S1[:,1])  #(13)\n",
    "    Psi = psi[:,0] * S2[:,0] + psi[:,1] * S2[:,1] + psi[:,2] * S2[:,2]\n",
    "        \n",
    "    # 4.4 The Final Topic Significance Score\n",
    "    return Psi*S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rec.autos' 'comp.sys.mac.hardware' 'rec.motorcycles' 'misc.forsale'\n",
      " 'comp.os.ms-windows.misc' 'alt.atheism' 'comp.graphics'\n",
      " 'rec.sport.baseball' 'rec.sport.hockey' 'sci.electronics' 'sci.space'\n",
      " 'talk.politics.misc' 'sci.med' 'talk.politics.mideast'\n",
      " 'soc.religion.christian' 'comp.windows.x' 'comp.sys.ibm.pc.hardware'\n",
      " 'talk.politics.guns' 'talk.religion.misc' 'sci.crypt']\n",
      "ding!\n"
     ]
    }
   ],
   "source": [
    "# Data import on its own line\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "import re, nltk, gensim, spacy\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
    "\n",
    "\n",
    "# Convert to list\n",
    "data = df.content.values.tolist()\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
    "    return texts_out\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# Run in terminal: python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only Noun, Adj, Verb, Adverb\n",
    "data_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=10,                        # minimum reqd occurences of a word \n",
    "                             stop_words='english',             # remove stop words\n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "                             # max_features=50000,             # max number of uniq words\n",
    "                            )\n",
    "\n",
    "data_vectorized = vectorizer.fit_transform(data_lemmatized)\n",
    "count_vector = data_vectorized\n",
    "print(count_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darin\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 10\n",
      "iteration: 2 of max_iter: 10\n",
      "iteration: 3 of max_iter: 10\n",
      "iteration: 4 of max_iter: 10\n",
      "iteration: 5 of max_iter: 10\n",
      "iteration: 6 of max_iter: 10\n",
      "iteration: 7 of max_iter: 10\n",
      "iteration: 8 of max_iter: 10\n",
      "iteration: 9 of max_iter: 10\n",
      "iteration: 10 of max_iter: 10\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "\n",
    "model = LatentDirichletAllocation(n_components=40, verbose=1, max_iter=10)\n",
    "documents = model.fit_transform(count_vector)\n",
    "components = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.63933507e-01,   1.33533406e-02,   6.10983757e-01,\n",
       "         7.02936283e-01,   2.46915124e-01,   6.70662010e-01,\n",
       "         1.09142404e-01,   4.68498627e-01,   5.28903571e-01,\n",
       "         5.21815685e-01,   7.51324916e-01,   1.91439730e+00,\n",
       "         5.11066012e-01,   2.34178734e-01,   3.54678009e-02,\n",
       "         3.63118915e-02,   1.30741900e-01,   1.74564463e-02,\n",
       "         1.25402261e-01,   8.12203702e-01,   9.53552281e-01,\n",
       "         7.82635785e-02,   7.39812345e-03,   3.57561425e-02,\n",
       "         3.23946929e-01,   6.05679727e-02,   1.67509197e-01,\n",
       "         4.70191340e+00,   8.37923752e-03,   6.51108086e-02,\n",
       "         1.26798709e+00,   8.06324794e-01,   9.69394624e-02,\n",
       "         2.68846611e-01,   6.52305982e-03,   7.61508645e-03,\n",
       "         2.42361176e-01,   2.34039220e+00,   1.43727585e-01,\n",
       "         1.70499331e-03])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TSR = TopicSignificanceRanking(count_vector, components, documents)\n",
    "TSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOPIC 11\n",
      "1.91439730422\n",
      "article 1769.06590181\n",
      "good 1999.86240061\n",
      "just 2259.86510284\n",
      "know 2005.70989528\n",
      "line 1438.34885061\n",
      "make 1617.74722005\n",
      "organization 1388.61242396\n",
      "people 1282.41939792\n",
      "problem 1245.40571663\n",
      "say 1358.32480497\n",
      "subject 1403.78671548\n",
      "thing 1293.70938044\n",
      "think 1715.40303975\n",
      "time 1573.80259998\n",
      "work 1264.17531147\n",
      "write 2077.16930039\n",
      "\n",
      "TOPIC 27\n",
      "4.70191340046\n",
      "article 3041.74114248\n",
      "host 4085.2561238\n",
      "line 5910.28352645\n",
      "nntp 4058.46052453\n",
      "organization 5588.77565848\n",
      "post 5132.15069163\n",
      "subject 5569.37318335\n",
      "university 2976.03924753\n",
      "write 3302.90625656\n",
      "\n",
      "TOPIC 37\n",
      "2.34039220363\n",
      "believe 1598.32551594\n",
      "christian 1626.79884448\n",
      "god 2649.40885783\n",
      "know 1708.16919911\n",
      "people 2104.7721552\n",
      "say 2942.33032433\n",
      "think 1767.30105075\n"
     ]
    }
   ],
   "source": [
    "for i, topic in enumerate(TSR >= np.max(TSR)/3):\n",
    "    if topic == True:\n",
    "        print()\n",
    "        print('TOPIC {}'.format(i))\n",
    "        print(TSR[i])\n",
    "        for j, val in enumerate(components[i,:] >= np.max(components[i,:])/2):\n",
    "            if val == True:\n",
    "                print(vectorizer.get_feature_names()[j], components[i,j])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validity Testing\n",
    "First, test multiple runs of same number of topics (dif max_iter) then on dif topic numbers, to see if topic numbers have a greater effect on the mean than the 'actual' significance of the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darin\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = [1+a for a in range(8)]\n",
    "Y = []\n",
    "Ym = {}\n",
    "for x in X:\n",
    "    model = LatentDirichletAllocation(n_components=40, max_iter=x)\n",
    "    documents = model.fit_transform(count_vector)\n",
    "    components = model.components_\n",
    "    TSR = TopicSignificanceRanking(count_vector, components, documents)\n",
    "    Ym[x] = np.sort(TSR)\n",
    "    print(np.mean(TSR), TSR)\n",
    "    Y.append(np.mean(TSR))\n",
    "\n",
    "plt.plot(X, Y)\n",
    "plt.xlabel('max_iterations')\n",
    "plt.ylabel('mean TSR')\n",
    "plt.show()\n",
    "print(Ym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
